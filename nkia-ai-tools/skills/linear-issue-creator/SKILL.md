---
name: linear-issue-creator
description: Create well-structured Linear issues with work-specific templates (Build/Deploy, Data, Evaluation, Feature Development, Feature Improvement, Refactoring, Research, Bug Fix). Supports both manual step-by-step input and automatic generation from meeting notes or natural language text with concrete DoD (Definition of Done) and AC (Acceptance Criteria). This skill should be used when users want to create a Linear issue for any type of work task.
---

# Linear Issue Creator

## Overview

Create well-structured Linear issues with appropriate work-specific templates, improved titles, automatic label application, and **concrete, measurable DoD/AC**.

**Two creation modes:**
1. **Manual Mode** - Step-by-step template-based input
2. **Auto Mode** - Automatic extraction from meeting notes or natural language text using LLM

The goal is to:
1. Select the right work template based on task type (or auto-detect)
2. Improve issue titles to be clear and concise
3. **Generate concrete and measurable DoD (Definition of Done) items**
4. **Generate concrete and measurable AC (Acceptance Criteria) items**
5. Automatically map work templates to Linear issue types
6. Automatically apply appropriate labels
7. Auto-assign cycle based on due date
8. Generate structured descriptions with DoD/AC
9. Allow users to review and edit before creating

## DoD vs AC

**Definition of Done (DoD):**
- Process completion evidence (í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ ì¦ë¹™)
- Examples: ì½”ë“œ ë¦¬ë·° ì™„ë£Œ, CI ë¹Œë“œ ì„±ê³µ, ë¦¬í¬íŠ¸ ì²¨ë¶€, ë¬¸ì„œ ì‘ì„±
- Focus on "what was done"
- **Format**: `- [ ] **[í•„ìˆ˜/ê³µí†µ/ì˜µì…”ë„]** [ì‘ì—… ë‚´ìš©] â†’ ê²°ê³¼ë¬¼: [êµ¬ì²´ì  ê²°ê³¼ë¬¼]`

**Acceptance Criteria (AC):**
- Quality standards for deliverables (ê²°ê³¼ë¬¼ í’ˆì§ˆ ê¸°ì¤€)
- Examples: ë©”íŠ¸ë¦­ ë‹¬ì„±, í…ŒìŠ¤íŠ¸ í†µê³¼, ì„±ëŠ¥ ê¸°ì¤€ ì¶©ì¡±
- Focus on "how good it is"
- **Format**: `- [ ] **[í•„ìˆ˜/ê³µí†µ/ì˜µì…”ë„]** [í’ˆì§ˆ ê¸°ì¤€] â†’ ê²°ê³¼ë¬¼: [ê²€ì¦ ê°€ëŠ¥í•œ ê²°ê³¼ë¬¼]`

**Make items concrete and measurable** with:
1. Checkbox format (`- [ ]`) for progress tracking
2. Priority tags (**[í•„ìˆ˜]**, **[ê³µí†µ]**, **[ì˜µì…”ë„]**)
3. Explicit deliverables (`â†’ ê²°ê³¼ë¬¼: ...`)
4. Template variables for context-specific values (`{{variable_name}}`)

## Work Templates and Issue Type Mapping

8 work templates are available, each automatically mapped to a Linear issue type:

| Work Template | Issue Type | Auto Labels |
|--------------|-----------|-------------|
| 1. ë¹Œë“œ/ë°°í¬ | Task | "build" |
| 2. ë°ì´í„° ì‘ì—… | Task | "task" |
| 3. í‰ê°€ | Task | "task" |
| 4. ìƒˆë¡œìš´ ê¸°ëŠ¥ ê°œë°œ | Feature | "feature" |
| 5. ê¸°ëŠ¥ ê°œì„  | Feature | "improvement" |
| 6. ë¦¬íŒ©í† ë§ | Feature | "improvement" |
| 7. ë¦¬ì„œì¹˜ | Research | "research" |
| 8. ë²„ê·¸ ìˆ˜ì • | Bug | "bug" |

**Available Linear labels:** bug, build, feature, improvement, research, task

---

## Auto Mode Workflow

**Use this mode when user provides meeting notes, natural language descriptions, or conversation transcripts.**

### Step 1 (Auto): Collect Natural Language Input

If not already provided, request the text:

```
íšŒì˜ë¡, ë©”ëª¨, ë˜ëŠ” ìì—°ì–´ë¡œ ì‘ì—… ë‚´ìš©ì„ ì…ë ¥í•´ì£¼ì„¸ìš”:

ì˜ˆì‹œ:
"ì˜¤ëŠ˜ íšŒì˜ì—ì„œ WSS ë°ì´í„°ì…‹ì´ ë¶€ì¡±í•˜ë‹¤ëŠ” ì–˜ê¸°ê°€ ë‚˜ì™”ì–´.
ë¡œí”„ ë‚œê¶Œ ë°ì´í„°ë¥¼ 11ì›” 25ì¼ê¹Œì§€ ìˆ˜ì§‘í•˜ê³  ì „ì²˜ë¦¬, ë¼ë²¨ë§ê¹Œì§€ ì™„ë£Œí•´ì•¼ í•´.
ì´ì„±ì›ë‹˜ì´ ë‹´ë‹¹í•˜ê¸°ë¡œ í–ˆê³ , Nkia-AI íŒ€ì—ì„œ ì§„í–‰."

ì…ë ¥:
```

### Step 2 (Auto): Extract Structured Data with LLM

**IMPORTANT: Use the Pydantic schema from `scripts/parse_natural_language.py` to extract structured JSON.**

1. Load the Pydantic schema (ParsedIssue model)
2. Analyze the natural language text
3. Extract information into structured JSON matching the schema:
   - Determine `template_type` (ë¹Œë“œ/ë°°í¬, ë°ì´í„° ì‘ì—…, í‰ê°€, etc.)
   - **Generate improved English `title` following the template patterns below**
   - Extract `team`, `project`, `assignee`, `priority`, `due_date` if mentioned
   - **Automatically populate `labels` based on template type**
   - Fill in template-specific fields (background, description, requirements, etc.)
   - **Generate concrete and measurable `dod_items` (Definition of Done)**
   - **Generate concrete and measurable `ac_items` (Acceptance Criteria)**

**English Title Patterns by Template Type:**
- **ë¹Œë“œ/ë°°í¬** â†’ `Deploy [target] to [environment]`
  - Example: "Deploy Auth API to Production"
- **ë°ì´í„° ì‘ì—…** â†’ `Process [data] for [purpose]`
  - Example: "Process Bank dataset for training"
- **í‰ê°€** â†’ `Evaluate [target] for [metric]`
  - Example: "Evaluate Fraud Detection Model for F1-score"
- **ìƒˆë¡œìš´ ê¸°ëŠ¥ ê°œë°œ** â†’ `Add [feature] to [purpose]`
  - Example: "Add trace analysis to track service calls"
- **ê¸°ëŠ¥ ê°œì„ ** â†’ `Improve [target] to [purpose]`
  - Example: "Improve query performance to reduce latency"
- **ë¦¬íŒ©í† ë§** â†’ `Refactor [target] to [purpose]`
  - Example: "Refactor parser module for maintainability"
- **ë¦¬ì„œì¹˜** â†’ `Research [topic] for [purpose]`
  - Example: "Research graph algorithms for trace analysis"
- **ë²„ê·¸ ìˆ˜ì •** â†’ `Fix [issue] in [target]`
  - Example: "Fix OAuth login failure for SSO users"

**Title Guidelines:**
- Use action verbs: Add, Fix, Improve, Optimize, Refactor, Deploy, Evaluate, Process, Research
- Length: 5-8 words (30-50 characters recommended)
- Be specific and concise
- Consider Linear's auto-generated branch names

**Label Selection by Template Type:**
- **ë¹Œë“œ/ë°°í¬** â†’ ["build"]
- **ë°ì´í„° ì‘ì—…** â†’ ["task"]
- **í‰ê°€** â†’ ["task"]
- **ìƒˆë¡œìš´ ê¸°ëŠ¥ ê°œë°œ** â†’ ["feature"]
- **ê¸°ëŠ¥ ê°œì„ ** â†’ ["improvement"]
- **ë¦¬íŒ©í† ë§** â†’ ["improvement"]
- **ë¦¬ì„œì¹˜** â†’ ["research"]
- **ë²„ê·¸ ìˆ˜ì •** â†’ ["bug"]

**DoD/AC Generation Guidelines:**

1. **Use checkbox format** (`- [ ]`):
   - All DoD and AC items must be checkboxes for progress tracking

2. **Specify deliverables explicitly** (using `â†’ ê²°ê³¼ë¬¼:`):
   - âŒ "í…ŒìŠ¤íŠ¸ ì™„ë£Œ"
   - âœ… "ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ 80% ì´ìƒ ë‹¬ì„± â†’ ê²°ê³¼ë¬¼: ì»¤ë²„ë¦¬ì§€ ë¦¬í¬íŠ¸ {{coverage_report_link}}"

3. **Use template variables for context-specific values**:
   - `{{dockerfile_path}}`, `{{ci_log_link}}`, `{{metric_name}}`, `{{threshold}}`, etc.

4. **Include evidence/proof requirements**:
   - "ì½”ë“œ ë¦¬ë·° ì™„ë£Œ â†’ ê²°ê³¼ë¬¼: PR ë§í¬ {{pr_link}} (ë¦¬ë·°ì–´ Approve)"
   - "CI ë¹Œë“œ ì„±ê³µ â†’ ê²°ê³¼ë¬¼: ë¹Œë“œ ë¡œê·¸ {{ci_log_link}}"

5. **Specify quantitative criteria with deliverables**:
   - "ë°ì´í„° {{record_count}}ê±´ ì´ìƒ ìˆ˜ì§‘ â†’ ê²°ê³¼ë¬¼: ìˆ˜ì§‘ ë°ì´í„° ê²½ë¡œ {{data_path}}"
   - "Null ë¹„ìœ¨ < {{null_threshold}}% â†’ ê²°ê³¼ë¬¼: í’ˆì§ˆ ë©”íŠ¸ë¦­ {{quality_report}}"
   - "API ì‘ë‹µ ì‹œê°„ < {{max_response_time}}ms â†’ ê²°ê³¼ë¬¼: ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼ {{perf_test_result}}"

**Template-Specific DoD/AC Patterns (í•µì‹¬ í•­ëª©ë§Œ):**

> ğŸ’¡ **ì›ì¹™**: DoD 2ê°œ + AC 2ê°œ = ì´ 4ê°œ ì´ë‚´ë¡œ ìœ ì§€. ê°€ì¥ ì¤‘ìš”í•œ ê²€ì¦ í¬ì¸íŠ¸ë§Œ í¬í•¨.

**ë¹Œë“œ/ë°°í¬:**
- DoD:
  - [ ] CI ë¹Œë“œ ì„±ê³µ ë° ì´ë¯¸ì§€ ë°°í¬ ì™„ë£Œ â†’ ê²°ê³¼ë¬¼: ë¹Œë“œ ë¡œê·¸ {{ci_log_link}}
  - [ ] ì½”ë“œ ë¦¬ë·° ì™„ë£Œ â†’ ê²°ê³¼ë¬¼: PR ë§í¬ {{pr_link}}
- AC:
  - [ ] {{environment}} í™˜ê²½ Healthcheck ì •ìƒ (200 OK) â†’ ê²°ê³¼ë¬¼: ì‘ë‹µ ë¡œê·¸ {{healthcheck_log}}
  - [ ] ë°°í¬ ë²„ì „({{release_version}}) ì •ìƒ ë°˜ì˜ í™•ì¸ â†’ ê²°ê³¼ë¬¼: ë²„ì „ ìŠ¤í¬ë¦°ìƒ· {{version_screenshot}}

**ë°ì´í„° ì‘ì—…:**
- DoD:
  - [ ] ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ â†’ ê²°ê³¼ë¬¼: ì €ì¥ ê²½ë¡œ {{storage_path}}
  - [ ] ì½”ë“œ ë¦¬ë·° ì™„ë£Œ â†’ ê²°ê³¼ë¬¼: PR ë§í¬ {{pr_link}}
- AC:
  - [ ] ëª©í‘œ ë°ì´í„° {{record_count}}ê±´ ì´ìƒ ìˆ˜ì§‘ â†’ ê²°ê³¼ë¬¼: ë°ì´í„° ê²½ë¡œ {{data_path}}
  - [ ] í’ˆì§ˆ ê¸°ì¤€ ì¶©ì¡± (Null < {{null_threshold}}%) â†’ ê²°ê³¼ë¬¼: í’ˆì§ˆ ë¦¬í¬íŠ¸ {{quality_report}}

**í‰ê°€:**
- DoD:
  - [ ] í‰ê°€ ì‹¤í–‰ ë° ê²°ê³¼ íŒŒì¼ ì²¨ë¶€ â†’ ê²°ê³¼ë¬¼: ê²°ê³¼ íŒŒì¼ {{eval_result_file}}
  - [ ] ê²°ê³¼ ë¶„ì„ ë¦¬í¬íŠ¸ ì‘ì„± â†’ ê²°ê³¼ë¬¼: ë¦¬í¬íŠ¸ {{report_link}}
- AC:
  - [ ] ëª©í‘œ ì§€í‘œ ë‹¬ì„± ({{metric_name}} â‰¥ {{threshold}}) â†’ ê²°ê³¼ë¬¼: ë©”íŠ¸ë¦­ ê²°ê³¼ {{metric_result}}
  - [ ] í…ŒìŠ¤íŠ¸ì…‹ ì •ë³´ ëª…ì‹œ (ë²„ì „: {{dataset_version}}) â†’ ê²°ê³¼ë¬¼: ë©”íƒ€ë°ì´í„° {{test_metadata}}

**ìƒˆë¡œìš´ ê¸°ëŠ¥ ê°œë°œ:**
- DoD:
  - [ ] ì½”ë“œ ë¦¬ë·° ì™„ë£Œ â†’ ê²°ê³¼ë¬¼: PR ë§í¬ {{pr_link}}
  - [ ] í…ŒìŠ¤íŠ¸ ì‘ì„± ë° í†µê³¼ â†’ ê²°ê³¼ë¬¼: í…ŒìŠ¤íŠ¸ ê²°ê³¼ {{test_result}}
- AC:
  - [ ] ìš”êµ¬ì‚¬í•­ ê¸°ëŠ¥ ì •ìƒ ë™ì‘ â†’ ê²°ê³¼ë¬¼: í…ŒìŠ¤íŠ¸ ì¦ë¹™ {{test_evidence}}
  - [ ] API ì‘ë‹µ ì‹œê°„ {{max_response_time}}ms ì´í•˜ â†’ ê²°ê³¼ë¬¼: ì„±ëŠ¥ ê²°ê³¼ {{perf_result}}

**ê¸°ëŠ¥ ê°œì„ :**
- DoD:
  - [ ] ì½”ë“œ ë¦¬ë·° ì™„ë£Œ â†’ ê²°ê³¼ë¬¼: PR ë§í¬ {{pr_link}}
  - [ ] ê°œì„  ì „/í›„ ë¹„êµ ì¸¡ì • â†’ ê²°ê³¼ë¬¼: ë¹„êµ ë¦¬í¬íŠ¸ {{comparison_report}}
- AC:
  - [ ] ëª©í‘œ ì§€í‘œ ë‹¬ì„± ({{metric_name}}: {{baseline}} â†’ {{goal}}) â†’ ê²°ê³¼ë¬¼: ë©”íŠ¸ë¦­ {{metrics}}

**ë¦¬íŒ©í† ë§:**
- DoD:
  - [ ] ì½”ë“œ ë¦¬ë·° ì™„ë£Œ â†’ ê²°ê³¼ë¬¼: PR ë§í¬ {{pr_link}}
  - [ ] ê¸°ì¡´ í…ŒìŠ¤íŠ¸ ì „ì²´ í†µê³¼ â†’ ê²°ê³¼ë¬¼: CI ë¡œê·¸ {{ci_log}}
- AC:
  - [ ] ê¸°ëŠ¥ ë™ì‘ ë™ì¼ (íšŒê·€ í…ŒìŠ¤íŠ¸ í†µê³¼) â†’ ê²°ê³¼ë¬¼: í…ŒìŠ¤íŠ¸ ê²°ê³¼ {{regression_result}}

**ë¦¬ì„œì¹˜:**
- DoD:
  - [ ] ì¡°ì‚¬ ê²°ê³¼ ë¬¸ì„œ ì‘ì„± â†’ ê²°ê³¼ë¬¼: ë¬¸ì„œ ë§í¬ {{summary_doc_link}}
  - [ ] íŒ€ ë¦¬ë·° ì™„ë£Œ â†’ ê²°ê³¼ë¬¼: ë¦¬ë·° ê¸°ë¡ {{review_record}}
- AC:
  - [ ] í•µì‹¬ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ì œì‹œ â†’ ê²°ê³¼ë¬¼: Q&A ì„¹ì…˜ {{qa_section}}
  - [ ] ë‹¤ìŒ ë‹¨ê³„ Action Item ëª…ì‹œ â†’ ê²°ê³¼ë¬¼: ì•¡ì…˜ ì•„ì´í…œ {{action_items}}

**ë²„ê·¸ ìˆ˜ì •:**
- DoD:
  - [ ] ì½”ë“œ ë¦¬ë·° ì™„ë£Œ â†’ ê²°ê³¼ë¬¼: PR ë§í¬ {{pr_link}}
  - [ ] ì¬ë°œ ë°©ì§€ í…ŒìŠ¤íŠ¸ ì¶”ê°€ â†’ ê²°ê³¼ë¬¼: í…ŒìŠ¤íŠ¸ ID {{test_case_id}}
- AC:
  - [ ] ì¬í˜„ ì‹œë„ ì‹œ ë²„ê·¸ ë¯¸ë°œìƒ â†’ ê²°ê³¼ë¬¼: ì¬í˜„ í…ŒìŠ¤íŠ¸ ê²°ê³¼ {{reproduction_test}}
  - [ ] ë°°í¬ í›„ ê´€ë ¨ ì—ëŸ¬ 0ê±´ â†’ ê²°ê³¼ë¬¼: ëª¨ë‹ˆí„°ë§ ë§í¬ {{monitoring_link}}

Example JSON output structure:
```json
{
  "metadata": {
    "template_type": "ë°ì´í„° ì‘ì—…",
    "title": "WSS ë°ì´í„°ì…‹ ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬",
    "team": "Nkia-AI",
    "project": null,
    "assignee": "ì´ì„±ì›",
    "priority": "Normal",
    "due_date": "2025-11-25",
    "labels": ["task"]
  },
  "template_data": {
    "background": "WSS ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ê³ í’ˆì§ˆ ë°ì´í„°ì…‹ êµ¬ì¶• í•„ìš”",
    "description": "WSS í•™ìŠµìš© ë°ì´í„° ìˆ˜ì§‘, ì •ì œ ë° ê²€ì¦ ì‘ì—…",
    "dod_items": [
      "ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ â†’ ê²°ê³¼ë¬¼: ì €ì¥ ê²½ë¡œ {{storage_path}}",
      "ì½”ë“œ ë¦¬ë·° ì™„ë£Œ â†’ ê²°ê³¼ë¬¼: PR ë§í¬ {{pr_link}}"
    ],
    "ac_items": [
      "ëª©í‘œ ë°ì´í„° {{record_count}}ê±´ ì´ìƒ ìˆ˜ì§‘ â†’ ê²°ê³¼ë¬¼: ë°ì´í„° ê²½ë¡œ {{data_path}}",
      "í’ˆì§ˆ ê¸°ì¤€ ì¶©ì¡± (Null < {{null_threshold}}%) â†’ ê²°ê³¼ë¬¼: í’ˆì§ˆ ë¦¬í¬íŠ¸ {{quality_report}}"
    ],
    "notes": "ë°ì´í„° í¬ë§·: JSONL, ìµœì†Œ 10,000ê°œ ìƒ˜í”Œ í™•ë³´"
  }
}
```

### Step 3 (Auto): Display Extracted Information and Offer Editing

Show the extracted information in a user-friendly format:

```
=== ìë™ ì¶”ì¶œëœ ì •ë³´ ===

**ë©”íƒ€ë°ì´í„°:**
- í…œí”Œë¦¿ íƒ€ì…: ë°ì´í„° ì‘ì—…
- ì œëª©: WSS ë°ì´í„°ì…‹ ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬
- íŒ€: Nkia-AI
- í”„ë¡œì íŠ¸: (ì—†ìŒ)
- ë‹´ë‹¹ì: ì´ì„±ì›
- ìš°ì„ ìˆœìœ„: Normal
- ë§ˆê°ì¼: 2025-11-25
- ë¼ë²¨: task

**ë°ì´í„° ì‘ì—… ìƒì„¸:**
- ë°°ê²½: WSS ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ê³ í’ˆì§ˆ ë°ì´í„°ì…‹ êµ¬ì¶• í•„ìš”
- ì‘ì—… ì„¤ëª…: WSS í•™ìŠµìš© ë°ì´í„° ìˆ˜ì§‘, ì •ì œ ë° ê²€ì¦ ì‘ì—…

**Definition of Done (DoD):**
- ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ â†’ ê²°ê³¼ë¬¼: ì €ì¥ ê²½ë¡œ {{storage_path}}
- ì½”ë“œ ë¦¬ë·° ì™„ë£Œ â†’ ê²°ê³¼ë¬¼: PR ë§í¬ {{pr_link}}

**Acceptance Criteria (AC):**
- ëª©í‘œ ë°ì´í„° {{record_count}}ê±´ ì´ìƒ ìˆ˜ì§‘ â†’ ê²°ê³¼ë¬¼: ë°ì´í„° ê²½ë¡œ {{data_path}}
- í’ˆì§ˆ ê¸°ì¤€ ì¶©ì¡± (Null < {{null_threshold}}%) â†’ ê²°ê³¼ë¬¼: í’ˆì§ˆ ë¦¬í¬íŠ¸ {{quality_report}}

- ì°¸ê³ ì‚¬í•­: ë°ì´í„° í¬ë§·: JSONL, ìµœì†Œ 10,000ê°œ ìƒ˜í”Œ í™•ë³´

========================

ì´ ì •ë³´ê°€ ë§ë‚˜ìš”?
1. ë„¤, ì´ëŒ€ë¡œ ì§„í–‰
2. ìˆ˜ì •í•˜ê¸°
3. ì²˜ìŒë¶€í„° ë‹¤ì‹œ

ì„ íƒ:
```

### Step 4 (Auto): Edit Mode (if user selects option 2)

Provide granular editing options:

```
ìˆ˜ì •í•  í•­ëª©ì„ ì„ íƒí•˜ì„¸ìš”:
1. ë©”íƒ€ë°ì´í„° (ì œëª©, íŒ€, ë‹´ë‹¹ì, ìš°ì„ ìˆœìœ„, ë§ˆê°ì¼, ë¼ë²¨)
2. í…œí”Œë¦¿ íƒ€ì… ë³€ê²½
3. ì‘ì—… ë‚´ìš© (ë°°ê²½, ì„¤ëª…, DoD, AC ë“±)
4. íŠ¹ì • í•„ë“œë§Œ ìˆ˜ì •

ì„ íƒ:
```

**For option 4 (specific field editing):**
```
ìˆ˜ì •í•  í•„ë“œëª… ì…ë ¥ (ì˜ˆ: title, priority, background, dod_items, ac_items):
í˜„ì¬ ê°’: [í˜„ì¬ ê°’ í‘œì‹œ]
ìƒˆë¡œìš´ ê°’:

ê³„ì† ìˆ˜ì •í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n):
```

Allow multiple edits in sequence until user confirms.

### Step 5 (Auto): Auto-assign Project Based on Content

Before assigning cycle, attempt to auto-assign a relevant project:

1. Fetch all active projects for the team using `mcp__linear__list_projects` with `team` parameter
2. Analyze the issue title and description to identify key terms and topics
3. Compare issue content with project names and descriptions
4. If a relevant project is found (high confidence match), store the project ID for issue creation
5. If no clear match or low confidence, leave project as null (do not force assignment)

**Project matching criteria:**
- Match keywords from issue title/description with project name
- Consider project status (prefer active projects over completed ones)
- Use semantic similarity for better matching (e.g., "API", "endpoint", "service" might match "API Development" project)
- Only assign if confidence is high (clear keyword overlap or semantic match)
- If multiple projects match, prefer the most recently updated project

**Example matching logic:**
- Issue: "Add authentication to user API" â†’ Project: "API Development" âœ…
- Issue: "Collect WSS dataset" â†’ Project: "WSS Model Training" âœ…
- Issue: "Fix login bug" â†’ No clear project match â†’ Leave as null âœ…

### Step 6 (Auto): Auto-assign Cycle Based on Due Date

If `due_date` is provided:
1. Fetch team cycles using `mcp__linear__list_cycles` with the team ID
2. Parse the due_date and compare with cycle date ranges (startsAt ~ endsAt)
3. Find the cycle where the due date falls within the range
4. Store the cycle ID for later use in issue creation

**Cycle matching logic:**
- Convert due_date (YYYY-MM-DD) to ISO format for comparison
- Compare with each cycle's startsAt and endsAt timestamps
- Select the cycle where: `startsAt <= due_date < endsAt`
- If no matching cycle found, skip cycle assignment (leave as null)

### Step 7 (Auto): Generate Markdown Description and Show Preview

Generate the final markdown description based on the template type and show preview:

```
=== ìƒì„±ë  ì´ìŠˆ ë¯¸ë¦¬ë³´ê¸° ===

ì œëª©: WSS ë°ì´í„°ì…‹ ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬
íƒ€ì…: Task
íŒ€: Nkia-AI
í”„ë¡œì íŠ¸: WSS Model Training (ìë™ ë§¤ì¹­ë¨) ë˜ëŠ” (ì—†ìŒ)
ìš°ì„ ìˆœìœ„: Normal
ë‹´ë‹¹ì: ì´ì„±ì›
ë§ˆê°ì¼: 2025-11-25
ì‚¬ì´í´: Cycle 2 (2025-11-23 ~ 2025-12-07)
ë¼ë²¨: task

--- ì„¤ëª… ---
## ë°°ê²½
WSS ëª¨ë¸ í•™ìŠµì„ ìœ„í•œ ê³ í’ˆì§ˆ ë°ì´í„°ì…‹ êµ¬ì¶• í•„ìš”

## ì‘ì—… ì„¤ëª…
WSS í•™ìŠµìš© ë°ì´í„° ìˆ˜ì§‘, ì •ì œ ë° ê²€ì¦ ì‘ì—…

## Definition of Done (DoD)
- [ ] ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ â†’ ê²°ê³¼ë¬¼: ì €ì¥ ê²½ë¡œ {{storage_path}}
- [ ] ì½”ë“œ ë¦¬ë·° ì™„ë£Œ â†’ ê²°ê³¼ë¬¼: PR ë§í¬ {{pr_link}}

## Acceptance Criteria (AC)
- [ ] ëª©í‘œ ë°ì´í„° {{record_count}}ê±´ ì´ìƒ ìˆ˜ì§‘ â†’ ê²°ê³¼ë¬¼: ë°ì´í„° ê²½ë¡œ {{data_path}}
- [ ] í’ˆì§ˆ ê¸°ì¤€ ì¶©ì¡± (Null < {{null_threshold}}%) â†’ ê²°ê³¼ë¬¼: í’ˆì§ˆ ë¦¬í¬íŠ¸ {{quality_report}}

## ì°¸ê³ ì‚¬í•­
ë°ì´í„° í¬ë§·: JSONL, ìµœì†Œ 10,000ê°œ ìƒ˜í”Œ í™•ë³´
--------------

ì´ëŒ€ë¡œ ìƒì„±í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n)
```

### Step 8 (Auto): Create Issue

Use `mcp__linear__create_issue` with all collected information including:
- Auto-assigned project ID (if found)
- Auto-assigned cycle ID (if found)
- Template-based labels

Then display the result URL with project information if assigned.

---

## Manual Mode Workflow

**IMPORTANT: Quick issue creation by collecting all necessary information at once.**

### Step 1 (Manual): Collect Basic Information at Once

First, fetch available teams using `mcp__linear__list_teams`, then present ALL basic questions in a single form:

```
Linear ì´ìŠˆë¥¼ ìƒì„±í•˜ê² ìŠµë‹ˆë‹¤. ë‹¤ìŒ ì •ë³´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”:

1. ì‘ì—… í…œí”Œë¦¿:
   1) ë¹Œë“œ/ë°°í¬
   2) ë°ì´í„° ì‘ì—…
   3) í‰ê°€
   4) ìƒˆë¡œìš´ ê¸°ëŠ¥ ê°œë°œ
   5) ê¸°ëŠ¥ ê°œì„ 
   6) ë¦¬íŒ©í† ë§
   7) ë¦¬ì„œì¹˜
   8) ë²„ê·¸ ìˆ˜ì •

2. íŒ€ ì´ë¦„: (ì‚¬ìš© ê°€ëŠ¥í•œ íŒ€: [íŒ€ ëª©ë¡])
3. í”„ë¡œì íŠ¸ ì´ë¦„: (ì„ íƒì‚¬í•­, ì—†ìœ¼ë©´ ì—”í„°)
4. ì´ìŠˆ ì œëª©:
5. ìš°ì„ ìˆœìœ„: (Urgent/High/Normal/Low, ì„ íƒì‚¬í•­)
6. ë‹´ë‹¹ì: (ì´ë¦„/ì´ë©”ì¼/'me', ì„ íƒì‚¬í•­)
7. ë§ˆê°ì¼: (YYYY-MM-DD, ì„ íƒì‚¬í•­)
```

**ì‚¬ìš©ìê°€ í•œ ë²ˆì— ë‹µë³€í•˜ëŠ” ì˜ˆì‹œ:**
```
1. 2
2. Nkia-AI
3.
4. WSS ë°ì´í„°ì…‹ ìˆ˜ì§‘
5. Normal
6. me
7. 2025-11-30
```

### Step 2 (Manual): Suggest Improved Title

Analyze the provided title and suggest an improved version following these guidelines:

**Good title characteristics:**
- **Action-oriented** (uses verbs)
- **Specific and concise** (avoid vague terms)
- **Clear scope and impact**

**Examples:**
- âŒ "ë¹Œë“œ" â†’ âœ… "RCA ì—ì´ì „íŠ¸ v2.0 ê°œë°œ ì„œë²„ ë°°í¬"
- âŒ "ë°ì´í„° ì‘ì—…" â†’ âœ… "WSS ë°ì´í„°ì…‹ ìˆ˜ì§‘ ë° ê²€ì¦"
- âŒ "í‰ê°€" â†’ âœ… "RCA ëª¨ë¸ ì •í™•ë„ í‰ê°€ (Acc@3)"
- âŒ "ë²„ê·¸ ìˆ˜ì •" â†’ âœ… "OAuth ë¡œê·¸ì¸ ì‹¤íŒ¨ ì˜¤ë¥˜ ìˆ˜ì •"

Present the improved title:
```
ì œì•ˆëœ ì œëª©: "WSS ë°ì´í„°ì…‹ ìˆ˜ì§‘ ë° ê²€ì¦"
ì´ ì œëª©ìœ¼ë¡œ ì§„í–‰í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n, ë˜ëŠ” ë‹¤ë¥¸ ì œëª© ì…ë ¥)
```

### Step 3 (Manual): Collect Template-Specific Details with DoD/AC

Based on the selected work template, request information including DoD/AC items. Use slash command examples as reference.

**For ë°ì´í„° ì‘ì—…:**
```
ë°ì´í„° ì‘ì—… ìƒì„¸ ì •ë³´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”:

1. ë°°ê²½ (ì™œ í•„ìš”?):
2. ì‘ì—… ì„¤ëª… (ì–´ë–¤ ë°ì´í„°? ëª©í‘œ í’ˆì§ˆì€?):

3. Definition of Done (DoD) - í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ ì¦ë¹™ (2ê°œ ê¶Œì¥):
   ì˜ˆì‹œ:
   - [ ] ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ â†’ ê²°ê³¼ë¬¼: ì €ì¥ ê²½ë¡œ {{storage_path}}
   - [ ] ì½”ë“œ ë¦¬ë·° ì™„ë£Œ â†’ ê²°ê³¼ë¬¼: PR ë§í¬ {{pr_link}}

ì…ë ¥:
- [ ]
- [ ]

4. Acceptance Criteria (AC) - ê²°ê³¼ë¬¼ í’ˆì§ˆ ê¸°ì¤€ (2ê°œ ê¶Œì¥):
   ì˜ˆì‹œ:
   - [ ] ëª©í‘œ ë°ì´í„° {{record_count}}ê±´ ì´ìƒ ìˆ˜ì§‘ â†’ ê²°ê³¼ë¬¼: ë°ì´í„° ê²½ë¡œ {{data_path}}
   - [ ] í’ˆì§ˆ ê¸°ì¤€ ì¶©ì¡± (Null < {{null_threshold}}%) â†’ ê²°ê³¼ë¬¼: í’ˆì§ˆ ë¦¬í¬íŠ¸ {{quality_report}}

ì…ë ¥:
- [ ]
- [ ]

5. ì°¸ê³ ì‚¬í•­ (ì„ íƒ, ë°ì´í„° ì†ŒìŠ¤/í¬ë§·/ì €ì¥ ìœ„ì¹˜):
```

### Step 4 (Manual): Apply Issue Type and Labels Automatically

Based on the selected work template, automatically:
1. Map to the appropriate Linear issue type (Task/Feature/Research/Bug)
2. Apply template-specific labels
3. Add content-based additional labels

### Step 4.5 (Manual): Auto-assign Project Based on Content

After collecting basic information and before cycle assignment, attempt to auto-assign a relevant project:

1. Fetch all active projects for the team using `mcp__linear__list_projects` with `team` parameter
2. Analyze the issue title and template details to identify key terms and topics
3. Compare issue content with project names and descriptions
4. If a relevant project is found (high confidence match), store the project ID for issue creation
5. If no clear match or low confidence, leave project as null (do not force assignment)

**Project matching criteria:**
- Match keywords from issue title/description with project name
- Consider project status (prefer active projects over completed ones)
- Use semantic similarity for better matching
- Only assign if confidence is high (clear keyword overlap or semantic match)
- If multiple projects match, prefer the most recently updated project

### Step 4.6 (Manual): Auto-assign Cycle Based on Due Date

If `due_date` is provided in Step 1:
1. Fetch team cycles using `mcp__linear__list_cycles` with the team ID
2. Parse the due_date and compare with cycle date ranges (startsAt ~ endsAt)
3. Find the cycle where the due date falls within the range
4. Store the cycle ID for later use in issue creation

**Cycle matching logic:**
- Convert due_date (YYYY-MM-DD) to ISO format for comparison
- Compare with each cycle's startsAt and endsAt timestamps
- Select the cycle where: `startsAt <= due_date < endsAt`
- If no matching cycle found, skip cycle assignment (leave as null)

### Step 5 (Manual): Show Preview and Confirm

Display the formatted issue preview with all collected information:

```
=== ìƒì„±ë  ì´ìŠˆ ë¯¸ë¦¬ë³´ê¸° ===

ì œëª©: [ê°œì„ ëœ ì œëª©]
íƒ€ì…: [ìë™ ë§¤í•‘ëœ ì´ìŠˆ íƒ€ì…]
íŒ€: [íŒ€]
í”„ë¡œì íŠ¸: [í”„ë¡œì íŠ¸ëª…] (ìë™ ë§¤ì¹­ë¨) ë˜ëŠ” (ì—†ìŒ)
ìš°ì„ ìˆœìœ„: [ìš°ì„ ìˆœìœ„]
ë‹´ë‹¹ì: [ë‹´ë‹¹ì]
ë§ˆê°ì¼: [ë§ˆê°ì¼]
ì‚¬ì´í´: [ìë™ ë°°ì •ëœ ì‚¬ì´í´] (if found)
ë¼ë²¨: [ìë™ ì„ íƒëœ ë¼ë²¨ë“¤]

--- ì„¤ëª… ---
## ë°°ê²½
[ì‚¬ìš©ì ì…ë ¥ ë‚´ìš©]

## ì‘ì—… ì„¤ëª…
[ì‚¬ìš©ì ì…ë ¥ ë‚´ìš©]

## Definition of Done (DoD)
- [ ] [DoD í•­ëª© 1] â†’ ê²°ê³¼ë¬¼: [ê²°ê³¼ë¬¼]
- [ ] [DoD í•­ëª© 2] â†’ ê²°ê³¼ë¬¼: [ê²°ê³¼ë¬¼]

## Acceptance Criteria (AC)
- [ ] [AC í•­ëª© 1] â†’ ê²°ê³¼ë¬¼: [ê²°ê³¼ë¬¼]
- [ ] [AC í•­ëª© 2] â†’ ê²°ê³¼ë¬¼: [ê²°ê³¼ë¬¼]

## ì°¸ê³ ì‚¬í•­
[ì‚¬ìš©ì ì…ë ¥ ë‚´ìš©]
--------------

ì´ëŒ€ë¡œ ìƒì„±í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n)
```

### Step 6 (Manual): Create the Issue

Use `mcp__linear__create_issue` with all collected information including:
- Auto-assigned project ID (if found)
- Auto-assigned cycle ID (if found)
- Template-based labels

Then display the result URL with project information if assigned.

---

## Important Guidelines

### 1. Title Improvement
Always suggest improved, clear titles with these characteristics:
- **Action-oriented** - Use verbs
- **Specific and concise** - Avoid vague terms
- **Clear scope and impact**

### 2. Label Selection
Automatically select labels based on work template type:
- **ë¹Œë“œ/ë°°í¬** â†’ "build"
- **ë°ì´í„° ì‘ì—…** â†’ "task"
- **í‰ê°€** â†’ "task"
- **ìƒˆë¡œìš´ ê¸°ëŠ¥ ê°œë°œ** â†’ "feature"
- **ê¸°ëŠ¥ ê°œì„ ** â†’ "improvement"
- **ë¦¬íŒ©í† ë§** â†’ "improvement"
- **ë¦¬ì„œì¹˜** â†’ "research"
- **ë²„ê·¸ ìˆ˜ì •** â†’ "bug"

**IMPORTANT:** Only use labels that exist in the Linear workspace: bug, build, feature, improvement, research, task. Do not create or suggest custom labels.

### 3. DoD/AC Management
- **Keep it minimal**: DoD 2ê°œ + AC 2ê°œ = ì´ 4ê°œ ì´ë‚´ ê¶Œì¥
- **Be concrete and measurable**: Use specific numbers, metrics, links
- **Use template variables**: `{{variable_name}}` for context-specific values
- **Include evidence**: Specify what proof is needed (links, reports, logs)

### 4. Content Focus
- Keep descriptions clear but not overly detailed
- Use DoD for process steps, AC for quality standards
- Add "ì°¸ê³ ì‚¬í•­" section only when there is relevant additional context

### 5. Project Auto-Assignment
- Fetch active projects for the team using `mcp__linear__list_projects`
- Analyze issue title/description for keywords and semantic matches
- Match with project names/descriptions using keyword overlap and semantic similarity
- Only assign if confidence is high (clear match)
- If no clear match, leave project as null - do not force assignment
- Prefer recently updated projects if multiple matches found

### 6. Validation
- Verify team exists using `mcp__linear__list_teams`
- Verify project exists using `mcp__linear__list_projects` (if specified by user or auto-assigned)
- Show final structure preview before creating issue

## Quick Process Principles

1. **Collect information in batches** - Present forms, not individual questions
2. **Mark optional fields clearly** - Use "(ì„ íƒ)" or "(ì„ íƒì‚¬í•­)"
3. **Provide examples** - Show users how to respond with concrete DoD/AC examples
4. **Single final confirmation** - Confirm only once after collecting all information

## Pydantic Schema for Auto Mode

When using Auto Mode, the LLM should extract structured data matching the Pydantic models defined in `scripts/parse_natural_language.py`.

**Key models:**
- `ParsedIssue` - Top-level container with metadata and template_data
- `IssueMetadata` - Contains template_type, title, team, project, assignee, priority, due_date, labels
- Template-specific models with `dod_items` and `ac_items`:
  - `BuildDeployTemplate` - For ë¹Œë“œ/ë°°í¬
  - `DataWorkTemplate` - For ë°ì´í„° ì‘ì—…
  - `EvaluationTemplate` - For í‰ê°€
  - `FeatureNewTemplate` - For ìƒˆë¡œìš´ ê¸°ëŠ¥ ê°œë°œ
  - `FeatureImproveTemplate` - For ê¸°ëŠ¥ ê°œì„ 
  - `RefactoringTemplate` - For ë¦¬íŒ©í† ë§
  - `ResearchTemplate` - For ë¦¬ì„œì¹˜
  - `BugTemplate` - For ë²„ê·¸ ìˆ˜ì •

All templates now include:
- `dod_items: List[str]` - Definition of Done items
- `ac_items: List[str]` - Acceptance Criteria items

The LLM should generate concrete, measurable items with template variables when extracting from natural language.
